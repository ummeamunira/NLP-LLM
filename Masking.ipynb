{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzHAIBaigiE5Xejoq+HRZq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ummeamunira/NLP-LLM/blob/main/Masking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate masking for MLM using the Hugging Face Transformers library:"
      ],
      "metadata": {
        "id": "GBRS39tZD1p0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDB7gfvKDvuD",
        "outputId": "f1805679-5ea5-4631-e30e-17d3249b4daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted token: the\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "inputs = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "# Mask a token (let's mask \"cat\")\n",
        "inputs['input_ids'][0, 1] = tokenizer.mask_token_id #inputs['input_ids'][0, 1]:[0]: Refers to the first (and only) sentence in the batch.[1]: Refers to the second token in that sentence (indexing starts at 0).\n",
        "\n",
        "# Run the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get the predictions for the masked token\n",
        "masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "predicted_token_id = outputs.logits[0, masked_index].argmax(axis=1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print(f\"Predicted token: {predicted_token}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the word \"cat\" in the sentence \"The cat sat on the mat.\" is masked, and the model predicts the masked token based on the context."
      ],
      "metadata": {
        "id": "0CKJOe2xED9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How attention mask works**"
      ],
      "metadata": {
        "id": "BTMqFETlGXTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example sentences\n",
        "sentences = [\"The cat sat.\", \"The cat sat on the mat.\"]\n",
        "\n",
        "# Tokenize sentences\n",
        "encoding = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Extract input IDs and attention mask\n",
        "input_ids = encoding['input_ids']\n",
        "attention_mask = encoding['attention_mask']\n",
        "\n",
        "print(\"Input IDs:\\n\", input_ids)\n",
        "print(\"Attention Mask:\\n\", attention_mask)\n",
        "\n",
        "# Run the model\n",
        "outputs = model(input_ids, attention_mask=attention_mask)\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "print(\"Last Hidden States Shape:\\n\", last_hidden_states.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7nDNUfJEGag",
        "outputId": "a0ace8ab-75a9-422c-f095-9d3e032ed7cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs:\n",
            " tensor([[  101,  1996,  4937,  2938,  1012,   102,     0,     0,     0],\n",
            "        [  101,  1996,  4937,  2938,  2006,  1996, 13523,  1012,   102]])\n",
            "Attention Mask:\n",
            " tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Last Hidden States Shape:\n",
            " torch.Size([2, 9, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input IDs: The tokenized input sentences with padding.\n",
        "\n",
        "Attention Mask: Indicates which tokens are real (1) and which are padding (0).\n",
        "\n",
        "Last Hidden States: The output tensor from the BERT model, with dimensions [batch_size, sequence_length, hidden_size].\n",
        "\n",
        "In this example, the attention mask ensures that the model does not consider the padding tokens (0 values in the mask) when computing attention, focusing only on the real tokens (1 values)."
      ],
      "metadata": {
        "id": "d2ReBF2PHWQh"
      }
    }
  ]
}